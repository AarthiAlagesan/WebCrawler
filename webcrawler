import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from collections import deque, defaultdict

class WebCrawler:
    def __init__(self, start_urls, max_pages=5):
        self.start_urls = start_urls
        self.max_pages = max_pages
        self.visited_urls = set()
        self.search_term = None
        self.graph = defaultdict(list)
        self.queue = deque()
        self.index = {}

    def set_search_term(self, term):
        """Set the search term for filtering URLs."""
        self.search_term = term.lower()

    def find_matching_urls(self):
        """Find URLs that match the search term."""
        matching_urls = []
        for url in self.start_urls:
            if self.search_term in url.lower():
                matching_urls.append(url)
        return matching_urls

    def crawl(self):
        """Crawl through the pages based on the matching URLs."""
        matching_urls = self.find_matching_urls()
        if not matching_urls:
            print(f"No matching URLs found for search term: {self.search_term}")
            return

        for url in matching_urls:
            self.queue.append(url)

        while self.queue and len(self.visited_urls) < self.max_pages:
            current_url = self.queue.popleft()
            self._crawl_page(current_url)

    def _crawl_page(self, url):
        """Crawl a single page and collect links."""
        if url in self.visited_urls:
            return

        print(f"Crawling: {url}")
        self.visited_urls.add(url)

        try:
            response = requests.get(url)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            # Index the content of the page
            self.index[url] = soup.get_text()

            for link in soup.find_all('a', href=True):
                full_url = urljoin(url, link['href'])

                # Only crawl URLs within the same domain
                if urlparse(full_url).netloc == urlparse(url).netloc:
                    if full_url not in self.visited_urls:
                        self.graph[url].append(full_url)
                        self.queue.append(full_url)
        except requests.exceptions.RequestException as e:
            print(f"Failed to crawl {url}: {e}")

    def display_results(self):
        """Display the crawled URLs and graph structure."""
        print("\nVisited URLs:")
        for url in sorted(self.visited_urls):
            print(url)

        print("\nGraph of URLs:")
        for url, links in sorted(self.graph.items()):
            print(f"{url}:")
            for link in sorted(links):
                print(f"  - {link}")

if __name__ == "__main__":
    # List of educational URLs to start crawling from
    start_urls = [
        "https://www.scaler.com",
        "https://www.guvi.in",
        "https://www.coursera.org",
        "https://www.udemy.com",
        "https://www.mygreatlearning.com",
        "https://www.w3schools.com",
        "https://www.javatpoint.com",
        "https://www.programiz.com",
        "https://www.tutorialspoint.com",
        "https://www.freecodecamp.org",
        "https://nptel.ac.in"
    ]

    # Create a WebCrawler instance
    crawler = WebCrawler(start_urls, max_pages=10)

    # Get search term from user input
    search_term = input("Enter search term to filter URLs: ")
    crawler.set_search_term(search_term)

    # Start crawling based on the search term
    crawler.crawl()

    # Display the results
    crawler.display_results()
